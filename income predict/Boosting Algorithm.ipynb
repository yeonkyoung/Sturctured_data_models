{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosing Algorithm\n",
    "## AdaBoost\n",
    "  AdaBoost는 1997년에 개발된 최초의 부스팅 알고리즘임. 직전 모델의 에러를 반영해 다음 모델에 weight를 주는 방식으로 학습한다. 이후 나온 GBM 계열 알고리즘이 더 성능이 뛰어나 요즘은 잘 쓰이지 않   는 알고리즘이지만 그 아이디어는 알아두면 좋을 듯하다.다음의 과정을 통해 학습이 이루어진다.\n",
    "\n",
    " \n",
    "\n",
    "   1. 전체 데이터에서 random sampling\n",
    "\n",
    " \n",
    "\n",
    "   2. 모든 sample 데이터 가중치 초기화\n",
    "\n",
    " \n",
    "\n",
    "   3. 첫번째 weak learner 만들고 학습 후 결과에 따라(맞음 or 틀림) 각 데이터에 가중치(C)와 해당 weak learner의 모델 가중치(w) 출력\n",
    "\n",
    " \n",
    "\n",
    "   4. 3번에서 구한 데이터 가중치(C)로 데이터를 update\n",
    "\n",
    " \n",
    "\n",
    "   5. update된 데이터로 두 번째 weak learner를 만들고 학습한다. 3~5 과정을 N번 반복한다.\n",
    "\n",
    " \n",
    "\n",
    "   6. N번의 iteration 후, N개의 weak learner들에 각각 모델 가중치(w)를 줘서 최종 모델을 만든다. (가중치 normalize 후 최종모델 도출)\n",
    "\n",
    "\n",
    "##  GBM(Gradient Boost)\n",
    "  Sequential 한 weak learner들을 residual을 줄이는 방향으로 결합하여 object function과의 loss를 줄여나가는 아이디어. 여기서 정의되는 residual이 negative gradient와 같은 의미를 지니게 되므로 gradient 부스팅이라는 이름이 붙었다. residual(잔차)은 실제값과 예측값의 차이.따라서 loss function을 MSE로 정의했을 때, negative gradient = residual 이 성립.\n",
    "  GBM은 residual을 줄이는 방향으로 weak learner들을 결합해 나간다. 위 그림을 보면 tree1, 2, 3가 각각 weak learner가 되고 각 모델에서 실제값(점)과 예측값(파란선)의 차이(residual)를 다음 모델에서 fitting 시키고 있음을 알 수 있다. 수식으로 써보면,\n",
    " - F(x) = A(x) + E   \n",
    "  F(x)가 object function, A(x)가 첫 번째 weak learner(tree1), E는 해당 모델에서의 error 즉 residual, 여기서 E(residual)를 다시 B(x)라는 weak learner로 fitting 시키면,\n",
    " - E = B(x) + E'\n",
    "  위 두 식을 합치면 다음과 같다   \n",
    " - F(x) = A(x) + B(x) + E'\n",
    "  이 과정을 N번 반복한다 residual이 줄어드는 방향으로!!\n",
    " - F(x) = A(x) + B(x) + C(x) + ... + E''\n",
    "  GBM에서도 weak learner로는 tree model을 사용한다. Adaboost 보다는 복잡한 tree를 사용하는데, 최종 tree node가 8~32개 정도로 짜는 게 일반적이다(파이썬에서 hyperparameter로 튜닝이 가능하다).\n",
    "  \n",
    "  우선 Input data set과 미분 가능한 loss function을 정의한다. GBM에서 loss function은 주로 MSE , L1 loss, Logistic loss가 사용된다. \n",
    "\n",
    " \n",
    "\n",
    "  1. F0(x)로 초기 예측값을 준다.\n",
    "\n",
    " \n",
    "\n",
    "  2. 학습을 진행하는데 1 ~ M 번 iteration을 돌린다.\n",
    "\n",
    "    (A) 위에서 정의한 loss를 미분해서 negative gradient = residual을 구한다.\n",
    "\n",
    "    (B) (A)에서 구한 residual을 target으로 하는 tree model(weak learner) 만들고 fitting 시킨다.\n",
    "\n",
    "    (C) loss를 최소로 하는 tree 결괏값을 출력한다. (이 과정은 생략되기도 한다)\n",
    "\n",
    "    (D) tree로 fitting 시켜서 구한 residual로 기존 예측값(F0(x))을 update 한다. 이때 gradient descent 알고리즘과 비슷하게 learning rate를 준다. \n",
    "\n",
    "  3. 위 과정을 iteration 횟수만큼 반복 한 뒤, 최종 예측값을 출력한다.\n",
    "\n",
    "\n",
    "\n",
    "##  XGBoost\n",
    "  GBM은 residual을 줄이는 방향으로 weak learner를 결합해 강력한 성능을 자랑하지만, 해당 train data에 residual을 계속 줄이니까 overfitting 되기 쉽다는 문제점이 있다. 이를 해결하기 위해 XGBoost는 GBM에 regularization term을 추가한 알고리즘이다. 또한 다양한 loss function을 지원해 task에 따른 유연한 튜닝이 가능하다는 장점이 있다.\n",
    "  - XGBoost = GBM + Regularization + 다양한 Loss function 지원.\n",
    "  - XGboost의 regularization term은 tree 복잡도가 증가할수록 loss에 페널티를 주는 방식으로 overfitting을 막고 있음.\n",
    "\n",
    "##  LightGBM\n",
    "부스팅 계열의 대부분 computational cost는 각 단계에서 weak learner인 best tree를 찾는데 쓰인다. 따라서 백만 개의 데이터를 XGBoost로 iteration=1000을 학습시킨 경우, 각 단계에서 tree를 fitting 시키기위해 백만개 데이터를 전부 scan 해야 한다. 해당 과정을 1000번 반복하니 computational cost가 너무 많이 들고 시간이 오래 걸린다. \n",
    "Light GBM은 이러한 높은 cost 문제를 histogram-based/GOSS(Gradient based One Side Sampling,기울기 기반 단측 표본추출)/EFB(탐욕 알고리즘)등의 알고리즘을 통해 tree를 구축하기 위한 scan 데이터 양을 줄임으로써 해결한다.\n",
    "\n",
    "#### GOSS(Gradient based One Side Sampling)\n",
    "\n",
    "\n",
    "#### EFB(탐욕 알고리즘)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
